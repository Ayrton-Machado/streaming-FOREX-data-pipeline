"""
Data Quality Service - Valida√ß√£o e limpeza de dados
Etapa 4: Detec√ß√£o de anomalias, corre√ß√£o de dados, relat√≥rios de qualidade
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
import logging
from dataclasses import dataclass

from app.domain.schemas import ValidatedOHLCV, MLReadyCandle
from app.core.config import settings

logger = logging.getLogger(__name__)


@dataclass
class DataQualityMetrics:
    """M√©tricas de qualidade dos dados"""
    total_records: int
    valid_records: int
    invalid_records: int
    duplicate_records: int
    missing_records: int
    outlier_records: int
    quality_score: float  # 0-100%
    
    # Detalhes espec√≠ficos
    price_inconsistencies: int
    volume_anomalies: int
    timestamp_gaps: int
    negative_prices: int
    zero_volume_records: int
    
    # Relat√≥rio temporal
    start_timestamp: datetime
    end_timestamp: datetime
    expected_records: int
    data_completeness: float  # %


@dataclass
class DataQualityIssue:
    """Representa um problema de qualidade identificado"""
    timestamp: datetime
    issue_type: str
    severity: str  # 'low', 'medium', 'high', 'critical'
    description: str
    suggested_action: str
    raw_value: Any
    corrected_value: Optional[Any] = None


class DataQualityService:
    """
    Servi√ßo para valida√ß√£o e melhoria da qualidade dos dados
    
    Filosofia SLC:
    - Simple: Valida√ß√µes claras baseadas em regras de mercado
    - Lovable: Corre√ß√µes autom√°ticas quando poss√≠vel
    - Complete: Cobertura total com relat√≥rios detalhados
    """
    
    def __init__(self):
        # Configura√ß√µes de valida√ß√£o para FOREX EUR/USD
        self.validation_config = {
            'price_range': {'min': 0.8, 'max': 2.0},      # EUR/USD range realista
            'daily_change_limit': 0.05,                    # 5% m√°ximo em 24h
            'spike_threshold': 0.02,                       # 2% spike threshold
            'volume_min': 0,                               # Volume m√≠nimo
            'max_gap_minutes': 60,                         # Gap m√°ximo entre velas
            'decimal_places': 5                            # Precis√£o FOREX
        }
        
        # Contadores de issues
        self.issues_found = []
        self.corrections_applied = 0
        
        logger.info("DataQualityService inicializado para FOREX EUR/USD")
    
    def validate_and_clean(
        self,
        candles: List[ValidatedOHLCV],
        auto_correct: bool = True,
        strict_mode: bool = False
    ) -> Tuple[List[ValidatedOHLCV], DataQualityMetrics]:
        """
        Valida e limpa dados com relat√≥rio de qualidade
        
        Args:
            candles: Lista de velas a validar
            auto_correct: Se deve tentar corrigir automaticamente
            strict_mode: Se deve ser mais rigoroso na valida√ß√£o
            
        Returns:
            Tupla (velas_limpas, m√©tricas_qualidade)
        """
        if not candles:
            logger.warning("Lista de velas vazia")
            return [], self._empty_metrics()
        
        logger.info(f"üîç Iniciando valida√ß√£o de qualidade: {len(candles)} velas")
        
        # Reset contadores
        self.issues_found = []
        self.corrections_applied = 0
        
        # Converte para DataFrame para an√°lise
        df = self._candles_to_dataframe(candles)
        original_count = len(df)
        
        # 1. Valida√ß√£o b√°sica de estrutura
        df = self._validate_basic_structure(df)
        
        # 2. Valida√ß√£o de pre√ßos
        df = self._validate_prices(df, auto_correct)
        
        # 3. Valida√ß√£o de volume
        df = self._validate_volume(df, auto_correct)
        
        # 4. Valida√ß√£o temporal
        df = self._validate_timestamps(df, auto_correct)
        
        # 5. Detec√ß√£o de duplicatas
        df = self._remove_duplicates(df)
        
        # 6. Detec√ß√£o de outliers
        df = self._detect_outliers(df, strict_mode)
        
        # 7. Valida√ß√£o de consist√™ncia OHLC
        df = self._validate_ohlc_consistency(df, auto_correct)
        
        # 8. Fill gaps se necess√°rio
        if auto_correct:
            df = self._fill_data_gaps(df)
        
        # Converte de volta para ValidatedOHLCV
        cleaned_candles = self._dataframe_to_candles(df, candles)
        
        # Calcula m√©tricas
        metrics = self._calculate_metrics(original_count, cleaned_candles, candles)
        
        logger.info(f"‚úÖ Valida√ß√£o conclu√≠da: {metrics.valid_records}/{original_count} velas v√°lidas")
        logger.info(f"üîß Corre√ß√µes aplicadas: {self.corrections_applied}")
        logger.info(f"üìä Score de qualidade: {metrics.quality_score:.1f}%")
        
        return cleaned_candles, metrics
    
    def _validate_basic_structure(self, df: pd.DataFrame) -> pd.DataFrame:
        """Valida√ß√£o b√°sica de estrutura dos dados"""
        
        # Verifica colunas obrigat√≥rias
        required_columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            issue = DataQualityIssue(
                timestamp=datetime.now(),
                issue_type='missing_columns',
                severity='critical',
                description=f"Colunas obrigat√≥rias ausentes: {missing_columns}",
                suggested_action='Verificar fonte de dados',
                raw_value=missing_columns
            )
            self.issues_found.append(issue)
            logger.error(f"‚ùå Colunas obrigat√≥rias ausentes: {missing_columns}")
        
        # Remove linhas com valores NaN em colunas cr√≠ticas
        initial_count = len(df)
        df = df.dropna(subset=['timestamp', 'open', 'high', 'low', 'close'])
        
        if len(df) < initial_count:
            removed_count = initial_count - len(df)
            issue = DataQualityIssue(
                timestamp=datetime.now(),
                issue_type='missing_values',
                severity='medium',
                description=f"Removidas {removed_count} linhas com valores ausentes",
                suggested_action='Verificar integridade da fonte',
                raw_value=removed_count
            )
            self.issues_found.append(issue)
            logger.warning(f"‚ö†Ô∏è Removidas {removed_count} linhas com valores NaN")
        
        return df
    
    def _validate_prices(self, df: pd.DataFrame, auto_correct: bool) -> pd.DataFrame:
        """Valida√ß√£o de pre√ßos"""
        
        # 1. Pre√ßos negativos
        negative_mask = (df[['open', 'high', 'low', 'close']] <= 0).any(axis=1)
        if negative_mask.any():
            count = negative_mask.sum()
            issue = DataQualityIssue(
                timestamp=datetime.now(),
                issue_type='negative_prices',
                severity='critical',
                description=f"Encontrados {count} registros com pre√ßos ‚â§ 0",
                suggested_action='Remover ou corrigir registros',
                raw_value=count
            )
            self.issues_found.append(issue)
            
            if auto_correct:
                df = df[~negative_mask]
                self.corrections_applied += count
                logger.warning(f"üîß Removidos {count} registros com pre√ßos ‚â§ 0")
        
        # 2. Pre√ßos fora do range realista
        price_cols = ['open', 'high', 'low', 'close']
        out_of_range_mask = (
            (df[price_cols] < self.validation_config['price_range']['min']) |
            (df[price_cols] > self.validation_config['price_range']['max'])
        ).any(axis=1)
        
        if out_of_range_mask.any():
            count = out_of_range_mask.sum()
            issue = DataQualityIssue(
                timestamp=datetime.now(),
                issue_type='price_out_of_range',
                severity='high',
                description=f"Pre√ßos fora do range {self.validation_config['price_range']}: {count} registros",
                suggested_action='Verificar se s√£o dados reais ou erros',
                raw_value=count
            )
            self.issues_found.append(issue)
            
            if auto_correct:
                df = df[~out_of_range_mask]
                self.corrections_applied += count
                logger.warning(f"üîß Removidos {count} registros com pre√ßos fora do range")
        
        # 3. Spikes de pre√ßo irreais
        df = self._detect_price_spikes(df, auto_correct)
        
        return df
    
    def _detect_price_spikes(self, df: pd.DataFrame, auto_correct: bool) -> pd.DataFrame:
        """Detecta spikes de pre√ßo irreais"""
        if len(df) < 3:
            return df
        
        # Calcula mudan√ßas percentuais
        df['price_change'] = df['close'].pct_change()
        
        # Identifica spikes
        spike_threshold = self.validation_config['spike_threshold']
        spike_mask = abs(df['price_change']) > spike_threshold
        
        if spike_mask.any():
            count = spike_mask.sum()
            issue = DataQualityIssue(
                timestamp=datetime.now(),
                issue_type='price_spikes',
                severity='medium',
                description=f"Detectados {count} spikes de pre√ßo >{spike_threshold*100}%",
                suggested_action='Verificar se s√£o movimentos reais do mercado',
                raw_value=count
            )
            self.issues_found.append(issue)
            
            if auto_correct:
                # Para spikes extremos (>10%), remove
                extreme_spikes = abs(df['price_change']) > 0.10
                if extreme_spikes.any():
                    extreme_count = extreme_spikes.sum()
                    df = df[~extreme_spikes]
                    self.corrections_applied += extreme_count
                    logger.warning(f"üîß Removidos {extreme_count} spikes extremos")
        
        # Remove coluna tempor√°ria
        df = df.drop('price_change', axis=1, errors='ignore')
        
        return df
    
    def _validate_volume(self, df: pd.DataFrame, auto_correct: bool) -> pd.DataFrame:
        """Valida√ß√£o de volume"""
        
        # Volume negativo
        negative_volume_mask = df['volume'] < 0
        if negative_volume_mask.any():
            count = negative_volume_mask.sum()
            issue = DataQualityIssue(
                timestamp=datetime.now(),
                issue_type='negative_volume',
                severity='high',
                description=f"Volume negativo encontrado: {count} registros",
                suggested_action='Corrigir para 0 ou valor absoluto',
                raw_value=count
            )
            self.issues_found.append(issue)
            
            if auto_correct:
                df.loc[negative_volume_mask, 'volume'] = 0
                self.corrections_applied += count
                logger.warning(f"üîß Corrigidos {count} volumes negativos para 0")
        
        # Volume zero (pode ser normal em FOREX)
        zero_volume_count = (df['volume'] == 0).sum()
        if zero_volume_count > len(df) * 0.5:  # Mais de 50%
            issue = DataQualityIssue(
                timestamp=datetime.now(),
                issue_type='excessive_zero_volume',
                severity='low',
                description=f"Muitos registros com volume zero: {zero_volume_count}/{len(df)}",
                suggested_action='Normal para FOREX, mas verificar fonte',
                raw_value=zero_volume_count
            )
            self.issues_found.append(issue)
        
        return df
    
    def _validate_timestamps(self, df: pd.DataFrame, auto_correct: bool) -> pd.DataFrame:
        """Valida√ß√£o temporal"""
        
        # Ordena por timestamp
        df = df.sort_values('timestamp').reset_index(drop=True)
        
        # Detecta timestamps duplicados
        duplicate_timestamps = df['timestamp'].duplicated()
        if duplicate_timestamps.any():
            count = duplicate_timestamps.sum()
            issue = DataQualityIssue(
                timestamp=datetime.now(),
                issue_type='duplicate_timestamps',
                severity='medium',
                description=f"Timestamps duplicados: {count}",
                suggested_action='Manter apenas o primeiro ou fazer m√©dia',
                raw_value=count
            )
            self.issues_found.append(issue)
            
            if auto_correct:
                # Mant√©m apenas o primeiro
                df = df[~duplicate_timestamps]
                self.corrections_applied += count
                logger.warning(f"üîß Removidos {count} timestamps duplicados")
        
        # Detecta gaps grandes
        if len(df) > 1:
            time_diffs = df['timestamp'].diff()
            max_gap = timedelta(minutes=self.validation_config['max_gap_minutes'])
            large_gaps = time_diffs > max_gap
            
            if large_gaps.any():
                count = large_gaps.sum()
                issue = DataQualityIssue(
                    timestamp=datetime.now(),
                    issue_type='large_time_gaps',
                    severity='low',
                    description=f"Gaps grandes detectados: {count} (>{self.validation_config['max_gap_minutes']}min)",
                    suggested_action='Pode ser normal (fim de semana, feriados)',
                    raw_value=count
                )
                self.issues_found.append(issue)
        
        return df
    
    def _remove_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:
        """Remove registros duplicados"""
        initial_count = len(df)
        
        # Remove duplicatas completas
        df = df.drop_duplicates()
        
        removed_count = initial_count - len(df)
        if removed_count > 0:
            issue = DataQualityIssue(
                timestamp=datetime.now(),
                issue_type='duplicate_records',
                severity='medium',
                description=f"Removidas {removed_count} duplicatas completas",
                suggested_action='Verificar processo de coleta',
                raw_value=removed_count
            )
            self.issues_found.append(issue)
            self.corrections_applied += removed_count
            logger.warning(f"üîß Removidas {removed_count} duplicatas")
        
        return df
    
    def _detect_outliers(self, df: pd.DataFrame, strict_mode: bool) -> pd.DataFrame:
        """Detecta outliers usando m√©todos estat√≠sticos"""
        if len(df) < 10:
            return df
        
        # M√©todo Z-score para pre√ßos
        price_cols = ['open', 'high', 'low', 'close']
        z_threshold = 3 if not strict_mode else 2.5
        
        outlier_mask = pd.Series([False] * len(df))
        
        for col in price_cols:
            z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())
            col_outliers = z_scores > z_threshold
            outlier_mask |= col_outliers
        
        if outlier_mask.any():
            count = outlier_mask.sum()
            issue = DataQualityIssue(
                timestamp=datetime.now(),
                issue_type='statistical_outliers',
                severity='low',
                description=f"Outliers detectados (Z-score>{z_threshold}): {count}",
                suggested_action='Verificar se s√£o movimentos reais ou erros',
                raw_value=count
            )
            self.issues_found.append(issue)
            
            # Marca outliers mas n√£o remove automaticamente
            df['is_outlier_detected'] = outlier_mask
        
        return df
    
    def _validate_ohlc_consistency(self, df: pd.DataFrame, auto_correct: bool) -> pd.DataFrame:
        """Valida consist√™ncia OHLC (High ‚â• Open,Close,Low; Low ‚â§ Open,Close,High)"""
        
        # High deve ser >= Open, Close, Low
        high_invalid = (
            (df['high'] < df['open']) |
            (df['high'] < df['close']) |
            (df['high'] < df['low'])
        )
        
        # Low deve ser <= Open, Close, High  
        low_invalid = (
            (df['low'] > df['open']) |
            (df['low'] > df['close']) |
            (df['low'] > df['high'])
        )
        
        inconsistent_mask = high_invalid | low_invalid
        
        if inconsistent_mask.any():
            count = inconsistent_mask.sum()
            issue = DataQualityIssue(
                timestamp=datetime.now(),
                issue_type='ohlc_inconsistency',
                severity='high',
                description=f"Inconsist√™ncias OHLC: {count} registros",
                suggested_action='Corrigir High/Low para manter consist√™ncia',
                raw_value=count
            )
            self.issues_found.append(issue)
            
            if auto_correct:
                # Corrige High e Low
                for idx in df[inconsistent_mask].index:
                    prices = [df.loc[idx, 'open'], df.loc[idx, 'close']]
                    df.loc[idx, 'high'] = max(df.loc[idx, 'high'], max(prices))
                    df.loc[idx, 'low'] = min(df.loc[idx, 'low'], min(prices))
                
                self.corrections_applied += count
                logger.warning(f"üîß Corrigidas {count} inconsist√™ncias OHLC")
        
        return df
    
    def _fill_data_gaps(self, df: pd.DataFrame) -> pd.DataFrame:
        """Preenche gaps de dados pequenos com interpola√ß√£o"""
        if len(df) < 2:
            return df
        
        # Detecta gaps de at√© 5 minutos
        df = df.sort_values('timestamp').reset_index(drop=True)
        time_diffs = df['timestamp'].diff()
        small_gaps = (time_diffs > timedelta(minutes=1)) & (time_diffs <= timedelta(minutes=5))
        
        if small_gaps.any():
            gap_count = small_gaps.sum()
            logger.info(f"üîÑ Preenchendo {gap_count} pequenos gaps com interpola√ß√£o")
            
            # Cria range temporal completo
            full_range = pd.date_range(
                start=df['timestamp'].min(),
                end=df['timestamp'].max(),
                freq='1min'
            )
            
            # Reindexes e interpola
            df = df.set_index('timestamp').reindex(full_range)
            
            # Interpola apenas pre√ßos, mant√©m volume como 0
            price_cols = ['open', 'high', 'low', 'close']
            df[price_cols] = df[price_cols].interpolate(method='linear')
            df['volume'] = df['volume'].fillna(0)
            
            # Marca registros interpolados
            df['is_gap_fill'] = df['volume'].isna()
            df['volume'] = df['volume'].fillna(0)
            
            # Reset index
            df = df.reset_index().rename(columns={'index': 'timestamp'})
            
            filled_count = df['is_gap_fill'].sum()
            if filled_count > 0:
                self.corrections_applied += filled_count
                logger.info(f"‚úÖ Preenchidos {filled_count} gaps com interpola√ß√£o")
        
        return df
    
    def _calculate_metrics(
        self,
        original_count: int,
        cleaned_candles: List[ValidatedOHLCV],
        original_candles: List[ValidatedOHLCV]
    ) -> DataQualityMetrics:
        """Calcula m√©tricas de qualidade"""
        
        valid_count = len(cleaned_candles)
        invalid_count = original_count - valid_count
        
        # Conta tipos espec√≠ficos de issues
        price_issues = len([i for i in self.issues_found if 'price' in i.issue_type])
        volume_issues = len([i for i in self.issues_found if 'volume' in i.issue_type])
        timestamp_issues = len([i for i in self.issues_found if 'timestamp' in i.issue_type or 'time' in i.issue_type])
        
        # Score de qualidade (0-100)
        quality_score = (valid_count / original_count) * 100 if original_count > 0 else 0
        
        # Aplica penalidades por tipos de issue
        critical_issues = len([i for i in self.issues_found if i.severity == 'critical'])
        high_issues = len([i for i in self.issues_found if i.severity == 'high'])
        
        quality_score -= (critical_issues * 10) + (high_issues * 5)
        quality_score = max(0, min(100, quality_score))
        
        # Timestamps
        start_ts = original_candles[0].timestamp if original_candles else datetime.now()
        end_ts = original_candles[-1].timestamp if original_candles else datetime.now()
        
        # Calcula completeness esperada (assumindo dados de 1 minuto)
        expected_duration = (end_ts - start_ts).total_seconds() / 60
        expected_records = int(expected_duration) if expected_duration > 0 else original_count
        data_completeness = (valid_count / expected_records) * 100 if expected_records > 0 else 100
        
        return DataQualityMetrics(
            total_records=original_count,
            valid_records=valid_count,
            invalid_records=invalid_count,
            duplicate_records=len([i for i in self.issues_found if 'duplicate' in i.issue_type]),
            missing_records=len([i for i in self.issues_found if 'missing' in i.issue_type]),
            outlier_records=len([i for i in self.issues_found if 'outlier' in i.issue_type]),
            quality_score=quality_score,
            price_inconsistencies=price_issues,
            volume_anomalies=volume_issues,
            timestamp_gaps=timestamp_issues,
            negative_prices=len([i for i in self.issues_found if i.issue_type == 'negative_prices']),
            zero_volume_records=0,  # Calculado separadamente se necess√°rio
            start_timestamp=start_ts,
            end_timestamp=end_ts,
            expected_records=expected_records,
            data_completeness=min(100, data_completeness)
        )
    
    def get_quality_report(self) -> Dict[str, Any]:
        """Retorna relat√≥rio detalhado de qualidade"""
        
        # Agrupa issues por tipo e severidade
        issues_by_type = {}
        issues_by_severity = {'low': 0, 'medium': 0, 'high': 0, 'critical': 0}
        
        for issue in self.issues_found:
            if issue.issue_type not in issues_by_type:
                issues_by_type[issue.issue_type] = []
            issues_by_type[issue.issue_type].append(issue)
            issues_by_severity[issue.severity] += 1
        
        return {
            "total_issues": len(self.issues_found),
            "corrections_applied": self.corrections_applied,
            "issues_by_type": {
                issue_type: len(issues) for issue_type, issues in issues_by_type.items()
            },
            "issues_by_severity": issues_by_severity,
            "detailed_issues": [
                {
                    "timestamp": issue.timestamp.isoformat(),
                    "type": issue.issue_type,
                    "severity": issue.severity,
                    "description": issue.description,
                    "action": issue.suggested_action,
                    "raw_value": str(issue.raw_value),
                    "corrected": issue.corrected_value is not None
                }
                for issue in self.issues_found
            ]
        }
    
    def _candles_to_dataframe(self, candles: List[ValidatedOHLCV]) -> pd.DataFrame:
        """Converte lista de velas para DataFrame"""
        data = []
        for candle in candles:
            data.append({
                'timestamp': candle.timestamp,
                'open': candle.open,
                'high': candle.high,
                'low': candle.low,
                'close': candle.close,
                'volume': candle.volume,
                'data_source': candle.data_source,
                'raw_symbol': candle.raw_symbol,
                'is_outlier': candle.is_outlier,
                'is_gap_fill': candle.is_gap_fill
            })
        
        return pd.DataFrame(data)
    
    def _dataframe_to_candles(
        self,
        df: pd.DataFrame,
        original_candles: List[ValidatedOHLCV]
    ) -> List[ValidatedOHLCV]:
        """Converte DataFrame de volta para ValidatedOHLCV"""
        candles = []
        
        for i, (_, row) in enumerate(df.iterrows()):
            # Pega dados originais se dispon√≠vel
            if i < len(original_candles):
                original = original_candles[i]
                data_source = original.data_source
                raw_symbol = original.raw_symbol
                validation = original.validation
            else:
                # Dados interpolados
                data_source = "interpolated"
                raw_symbol = "EURUSD"
                validation = {"interpolated": True}
            
            candle = ValidatedOHLCV(
                timestamp=row['timestamp'],
                open=row['open'],
                high=row['high'],
                low=row['low'],
                close=row['close'],
                volume=row['volume'],
                data_source=data_source,
                raw_symbol=raw_symbol,
                validation=validation,
                is_outlier=row.get('is_outlier_detected', False),
                is_gap_fill=row.get('is_gap_fill', False)
            )
            candles.append(candle)
        
        return candles
    
    def _empty_metrics(self) -> DataQualityMetrics:
        """Retorna m√©tricas vazias"""
        return DataQualityMetrics(
            total_records=0,
            valid_records=0,
            invalid_records=0,
            duplicate_records=0,
            missing_records=0,
            outlier_records=0,
            quality_score=0.0,
            price_inconsistencies=0,
            volume_anomalies=0,
            timestamp_gaps=0,
            negative_prices=0,
            zero_volume_records=0,
            start_timestamp=datetime.now(),
            end_timestamp=datetime.now(),
            expected_records=0,
            data_completeness=0.0
        )


# Factory function
def get_data_quality_service() -> DataQualityService:
    """Retorna inst√¢ncia de DataQualityService"""
    return DataQualityService()